apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: news-pipeline-template
  namespace: production
spec:
  serviceAccountName: argo-workflow
  entrypoint: pipeline

  # Define reusable templates for the pipeline
  templates:
  # Main pipeline orchestration (DAG) â€” allows overlap between discovery,
  # verification and extraction via polling-based gates.
  - name: pipeline
    inputs:
      parameters:
      - name: dataset
      - name: source-limit
        value: "10000"  # Default: effectively unlimited (process all sources)
      - name: max-articles
      - name: days-back
      - name: verify-batch-size
      - name: verify-max-batches
      - name: extract-limit
      - name: extract-batches
      - name: inter-request-min
      - name: inter-request-max
      - name: batch-sleep
      - name: captcha-backoff-base
      - name: captcha-backoff-max
        value: "7200"
      - name: min-candidates
        value: "50"
      - name: candidate-wait-seconds
        value: "300"  # 5 minutes
      - name: min-verified
        value: "25"
      - name: verified-wait-seconds
        value: "300"
    dag:
      tasks:
      - name: discover-urls
        template: discovery-step
        arguments:
          parameters:
          - name: dataset
            value: "{{inputs.parameters.dataset}}"
          - name: source-limit
            value: "{{inputs.parameters.source-limit}}"
          - name: max-articles
            value: "{{inputs.parameters.max-articles}}"
          - name: days-back
            value: "{{inputs.parameters.days-back}}"

      # Start a parallel wait task that will poll the DB for new candidate
      # links and only succeed when either min-candidates are present or
      # the candidate-wait-seconds timeout is reached.
      - name: wait-for-candidates
        template: wait-for-candidates
        arguments:
          parameters:
          - name: min-candidates
            value: "{{inputs.parameters.min-candidates}}"
          - name: timeout-seconds
            value: "{{inputs.parameters.candidate-wait-seconds}}"

      - name: verify-urls
        template: verification-step
        depends: "wait-for-candidates"
        arguments:
          parameters:
          - name: batch-size
            value: "{{inputs.parameters.verify-batch-size}}"
          - name: max-batches
            value: "{{inputs.parameters.verify-max-batches}}"
          - name: inter-request-min
            value: "{{inputs.parameters.inter-request-min}}"
          - name: inter-request-max
            value: "{{inputs.parameters.inter-request-max}}"

      - name: wait-for-verified
        template: wait-for-verified
        depends: "verify-urls"
        arguments:
          parameters:
          - name: min-verified
            value: "{{inputs.parameters.min-verified}}"
          - name: timeout-seconds
            value: "{{inputs.parameters.verified-wait-seconds}}"

      - name: extract-content
        template: extraction-step
        depends: "wait-for-verified"
        arguments:
          parameters:
          - name: dataset
            value: "{{inputs.parameters.dataset}}"
          - name: limit
            value: "{{inputs.parameters.extract-limit}}"
          - name: batches
            value: "{{inputs.parameters.extract-batches}}"
          - name: inter-request-min
            value: "{{inputs.parameters.inter-request-min}}"
          - name: inter-request-max
            value: "{{inputs.parameters.inter-request-max}}"
          - name: batch-sleep
            value: "{{inputs.parameters.batch-sleep}}"
          - name: captcha-backoff-base
            value: "{{inputs.parameters.captcha-backoff-base}}"
          - name: captcha-backoff-max
            value: "{{inputs.parameters.captcha-backoff-max}}"
  
  # Discovery step template
  - name: discovery-step
    inputs:
      parameters:
      - name: dataset
      - name: source-limit
        value: "10000"  # Default: effectively unlimited
      - name: max-articles
      - name: days-back
    metadata:
      labels:
        stage: discovery
    retryStrategy:
      limit: 2
      retryPolicy: "OnFailure"
      backoff:
        duration: "5m"
        factor: 2
        maxDuration: "20m"
    container:
      image: us-central1-docker.pkg.dev/mizzou-news-crawler/mizzou-crawler/processor:latest
      imagePullPolicy: Always
      command:
        - python
        - -m
        - src.cli.cli_modular
        - discover-urls
        - --dataset
        - "{{inputs.parameters.dataset}}"
        - --source-limit
        - "{{inputs.parameters.source-limit}}"
        - --max-articles
        - "{{inputs.parameters.max-articles}}"
        - --days-back
        - "{{inputs.parameters.days-back}}"
      envFrom:
      - secretRef:
          name: origin-proxy-credentials
      env:
      # Database configuration
      - name: DATABASE_ENGINE
        value: "postgresql+psycopg2"
      - name: DATABASE_HOST
        value: "127.0.0.1"
      - name: DATABASE_PORT
        value: "5432"
      - name: DATABASE_USER
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: username
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: password
      - name: DATABASE_NAME
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: database
      # Explicit DATABASE_URL to prevent SQLite fallback (Kubernetes will expand $(VAR) references)
      - name: DATABASE_URL
        value: "$(DATABASE_ENGINE)://$(DATABASE_USER):$(DATABASE_PASSWORD)@$(DATABASE_HOST):$(DATABASE_PORT)/$(DATABASE_NAME)"
      # Cloud SQL Connector
      - name: USE_CLOUD_SQL_CONNECTOR
        value: "true"
      - name: CLOUD_SQL_INSTANCE
        value: "mizzou-news-crawler:us-central1:mizzou-db-prod"
      # Proxy configuration
      - name: PROXY_PROVIDER
        value: "decodo"
      - name: USE_ORIGIN_PROXY
        value: "false"
      - name: NO_PROXY
        value: "localhost,127.0.0.1,metadata.google.internal,huggingface.co,*.huggingface.co"
      resources:
        requests:
          cpu: 200m
          memory: 2Gi
        limits:
          cpu: 1000m
          memory: 4Gi
  
  # Verification step template
  - name: verification-step
    inputs:
      parameters:
      - name: batch-size
      - name: max-batches
      - name: inter-request-min
      - name: inter-request-max
    metadata:
      labels:
        stage: verification
    retryStrategy:
      limit: 2
      retryPolicy: "OnFailure"
      backoff:
        duration: "5m"
        factor: 2
        maxDuration: "20m"
    container:
      image: us-central1-docker.pkg.dev/mizzou-news-crawler/mizzou-crawler/processor:latest
      imagePullPolicy: Always
      command:
        - python
        - -m
        - src.cli.cli_modular
        - verify-urls
        - --batch-size
        - "{{inputs.parameters.batch-size}}"
        - --max-batches
        - "{{inputs.parameters.max-batches}}"
      envFrom:
      - secretRef:
          name: origin-proxy-credentials
      env:
      # Database configuration
      - name: DATABASE_ENGINE
        value: "postgresql+psycopg2"
      - name: DATABASE_HOST
        value: "127.0.0.1"
      - name: DATABASE_PORT
        value: "5432"
      - name: DATABASE_USER
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: username
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: password
      - name: DATABASE_NAME
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: database
      # Explicit DATABASE_URL to prevent SQLite fallback (Kubernetes will expand $(VAR) references)
      - name: DATABASE_URL
        value: "$(DATABASE_ENGINE)://$(DATABASE_USER):$(DATABASE_PASSWORD)@$(DATABASE_HOST):$(DATABASE_PORT)/$(DATABASE_NAME)"
      # Cloud SQL Connector
      - name: USE_CLOUD_SQL_CONNECTOR
        value: "true"
      - name: CLOUD_SQL_INSTANCE
        value: "mizzou-news-crawler:us-central1:mizzou-db-prod"
      # Proxy configuration
      - name: PROXY_PROVIDER
        value: "decodo"
      - name: USE_ORIGIN_PROXY
        value: "false"
      - name: NO_PROXY
        value: "localhost,127.0.0.1,metadata.google.internal,huggingface.co,*.huggingface.co"
      # Rate limiting for verification
      - name: INTER_REQUEST_MIN
        value: "{{inputs.parameters.inter-request-min}}"
      - name: INTER_REQUEST_MAX
        value: "{{inputs.parameters.inter-request-max}}"
      resources:
        requests:
          cpu: 250m
          memory: 1Gi
        limits:
          cpu: 1000m
          memory: 3Gi
  
  # Extraction step template
  - name: extraction-step
    inputs:
      parameters:
      - name: dataset
      - name: limit
      - name: batches
      - name: inter-request-min
      - name: inter-request-max
      - name: batch-sleep
      - name: captcha-backoff-base
      - name: captcha-backoff-max
    metadata:
      labels:
        stage: extraction
    retryStrategy:
      limit: 2
      retryPolicy: "OnFailure"
      backoff:
        duration: "5m"
        factor: 2
        maxDuration: "20m"
    container:
      image: us-central1-docker.pkg.dev/mizzou-news-crawler/mizzou-crawler/processor:latest
      imagePullPolicy: Always
      command:
        - python
        - -m
        - src.cli.cli_modular
        - extract
        - --dataset
        - "{{inputs.parameters.dataset}}"
        - --limit
        - "{{inputs.parameters.limit}}"
        - --batches
        - "{{inputs.parameters.batches}}"
      envFrom:
      - secretRef:
          name: origin-proxy-credentials
      env:
      # Database configuration
      - name: DATABASE_ENGINE
        value: "postgresql+psycopg2"
      - name: DATABASE_HOST
        value: "127.0.0.1"
      - name: DATABASE_PORT
        value: "5432"
      - name: DATABASE_USER
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: username
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: password
      - name: DATABASE_NAME
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: database
      # Explicit DATABASE_URL to prevent SQLite fallback (Kubernetes will expand $(VAR) references)
      - name: DATABASE_URL
        value: "$(DATABASE_ENGINE)://$(DATABASE_USER):$(DATABASE_PASSWORD)@$(DATABASE_HOST):$(DATABASE_PORT)/$(DATABASE_NAME)"
      # Cloud SQL Connector
      - name: USE_CLOUD_SQL_CONNECTOR
        value: "true"
      - name: CLOUD_SQL_INSTANCE
        value: "mizzou-news-crawler:us-central1:mizzou-db-prod"
      # Proxy configuration
      - name: PROXY_PROVIDER
        value: "decodo"
      - name: USE_ORIGIN_PROXY
        value: "false"
      - name: SELENIUM_PROXY
        valueFrom:
          secretKeyRef:
            name: origin-proxy-credentials
            key: selenium-proxy-url
      # Rate limiting
      - name: INTER_REQUEST_MIN
        value: "{{inputs.parameters.inter-request-min}}"
      - name: INTER_REQUEST_MAX
        value: "{{inputs.parameters.inter-request-max}}"
      - name: BATCH_SLEEP_SECONDS
        value: "{{inputs.parameters.batch-sleep}}"
      - name: CAPTCHA_BACKOFF_BASE
        value: "{{inputs.parameters.captcha-backoff-base}}"
      - name: CAPTCHA_BACKOFF_MAX
        value: "{{inputs.parameters.captcha-backoff-max}}"
      # User agent rotation
      - name: UA_ROTATE_BASE
        value: "4"
      - name: UA_ROTATE_JITTER
        value: "0.25"
      # Decodo IP rotation
      - name: DECODO_ROTATE_IP
        value: "true"
      # Bypass proxy for internal services
      - name: NO_PROXY
        value: "localhost,127.0.0.1,metadata.google.internal,huggingface.co,*.huggingface.co"
      resources:
        requests:
          cpu: 250m
          memory: 1Gi
        limits:
          cpu: 1000m
          memory: 3Gi

  # Wait template: poll DB until a minimum number of candidate links exist
  - name: wait-for-candidates
    inputs:
      parameters:
      - name: min-candidates
      - name: timeout-seconds
    container:
      image: us-central1-docker.pkg.dev/mizzou-news-crawler/mizzou-crawler/processor:latest
      command: ["/bin/sh","-c"]
      args:
        - |
          python - <<'PY'
            import time
            import sys
            from datetime import datetime, timedelta
            from src import config
            from sqlalchemy import create_engine, text

            min_candidates = int('{{inputs.parameters.min-candidates}}')
            timeout = int('{{inputs.parameters.timeout-seconds}}')
            deadline = datetime.utcnow() + timedelta(seconds=timeout)

            if getattr(config, 'USE_CLOUD_SQL_CONNECTOR', False) and getattr(config, 'CLOUD_SQL_INSTANCE', None):
                from src.models.cloud_sql_connector import create_cloud_sql_engine
                engine = create_cloud_sql_engine(
                    instance_connection_name=config.CLOUD_SQL_INSTANCE,
                    user=config.DATABASE_USER,
                    password=config.DATABASE_PASSWORD,
                    database=config.DATABASE_NAME,
                )
            else:
                engine = create_engine(config.DATABASE_URL)

            while True:
                try:
                    with engine.connect() as conn:
                        r = conn.execute(text("SELECT COUNT(*) FROM candidate_links WHERE created_at >= now() - interval '1 hour'"))
                        count = r.scalar() or 0
                        print(f"candidates_in_last_hour={count}")
                        if count >= min_candidates:
                            print("threshold met")
                            sys.exit(0)
                except Exception as e:
                    print(f"db check error: {e}")

                if datetime.utcnow() >= deadline:
                    print("timeout reached; proceeding")
                    sys.exit(0)

                time.sleep(15)
          PY
      env:
      - name: DATABASE_ENGINE
        value: "postgresql+psycopg2"
      - name: DATABASE_HOST
        value: "127.0.0.1"
      - name: DATABASE_PORT
        value: "5432"
      - name: DATABASE_USER
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: username
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: password
      - name: DATABASE_NAME
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: database
      - name: DATABASE_URL
        value: "$(DATABASE_ENGINE)://$(DATABASE_USER):$(DATABASE_PASSWORD)@$(DATABASE_HOST):$(DATABASE_PORT)/$(DATABASE_NAME)"
      - name: USE_CLOUD_SQL_CONNECTOR
        value: "true"
      - name: CLOUD_SQL_INSTANCE
        value: "mizzou-news-crawler:us-central1:mizzou-db-prod"

  # Wait template: poll DB until a minimum number of verified articles exist
  - name: wait-for-verified
    inputs:
      parameters:
      - name: min-verified
      - name: timeout-seconds
    container:
      image: us-central1-docker.pkg.dev/mizzou-news-crawler/mizzou-crawler/processor:latest
      command: ["/bin/sh","-c"]
      args:
        - |
          python - <<'PY'
          import time
          import sys
          from datetime import datetime, timedelta
          from src import config
          from sqlalchemy import create_engine, text

          min_verified = int('{{inputs.parameters.min-verified}}')
          timeout = int('{{inputs.parameters.timeout-seconds}}')
          deadline = datetime.utcnow() + timedelta(seconds=timeout)

          if getattr(config, 'USE_CLOUD_SQL_CONNECTOR', False) and getattr(config, 'CLOUD_SQL_INSTANCE', None):
              from src.models.cloud_sql_connector import create_cloud_sql_engine
              engine = create_cloud_sql_engine(
                  instance_connection_name=config.CLOUD_SQL_INSTANCE,
                  user=config.DATABASE_USER,
                  password=config.DATABASE_PASSWORD,
                  database=config.DATABASE_NAME,
              )
          else:
              engine = create_engine(config.DATABASE_URL)

          while True:
              try:
                  with engine.connect() as conn:
                      r = conn.execute(text("SELECT COUNT(*) FROM candidate_links WHERE verification_status = 'verified' AND verified_at >= now() - interval '1 hour'"))
                      count = r.scalar() or 0
                      print(f"verified_in_last_hour={count}")
                      if count >= min_verified:
                          print("verified threshold met")
                          sys.exit(0)
              except Exception as e:
                  print(f"db check error: {e}")

              if datetime.utcnow() >= deadline:
                  print("timeout reached; proceeding")
                  sys.exit(0)

              time.sleep(15)
          PY
      env:
      - name: DATABASE_ENGINE
        value: "postgresql+psycopg2"
      - name: DATABASE_HOST
        value: "127.0.0.1"
      - name: DATABASE_PORT
        value: "5432"
      - name: DATABASE_USER
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: username
      - name: DATABASE_PASSWORD
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: password
      - name: DATABASE_NAME
        valueFrom:
          secretKeyRef:
            name: cloudsql-db-credentials
            key: database
      - name: DATABASE_URL
        value: "$(DATABASE_ENGINE)://$(DATABASE_USER):$(DATABASE_PASSWORD)@$(DATABASE_HOST):$(DATABASE_PORT)/$(DATABASE_NAME)"
      - name: USE_CLOUD_SQL_CONNECTOR
        value: "true"
      - name: CLOUD_SQL_INSTANCE
        value: "mizzou-news-crawler:us-central1:mizzou-db-prod"
