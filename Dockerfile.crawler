# Multi-stage Dockerfile for Crawler services
# Handles discovery, extraction, and verification
# Optimized using shared base image

# Use ARG to allow override of base image location
ARG BASE_IMAGE=mizzou-base:latest

# Stage 1: Base with additional crawler system dependencies
FROM ${BASE_IMAGE} AS base-crawler

# Install system dependencies for browser automation (Playwright, Selenium)
# Note: Chromium was removed from Debian Bookworm repos - use Debian testing repos or Snap
# Install base libraries + chromium from testing repos + pre-create cache directories

RUN apt-get clean && \
    rm -rf /var/lib/apt/lists/* && \
    apt-get update && \
    apt-get install -y --no-install-recommends \
        fonts-liberation libnss3 libxss1 xdg-utils libxdamage1 libgconf-2-4 libx11-xcb1 wget ca-certificates && \
    # Try to install chromium from bookworm-backports, then testing, then snapshot
    apt-get install -y --no-install-recommends chromium 2>/dev/null || \
    (grep -q "bookworm-backports" /etc/apt/sources.list /etc/apt/sources.list.d/* 2>/dev/null || \
     echo "deb http://deb.debian.org/debian bookworm-backports main contrib non-free" >> /etc/apt/sources.list.d/backports.list && \
     apt-get update && \
     apt-get install -y --no-install-recommends -t bookworm-backports chromium) || \
    (echo "deb http://deb.debian.org/debian testing main contrib non-free" >> /etc/apt/sources.list.d/testing.list && \
     apt-get update && \
     apt-get install -y --no-install-recommends chromium) || \
    echo "WARNING: chromium package not available, relying on undetected-chromedriver download at runtime" && \
    rm -rf /var/lib/apt/lists/* && \
    # Pre-create writable directories for undetected-chromedriver browser cache and chromedriver binary storage
    mkdir -p /opt/chromedriver-cache /home/appuser/.wdm /tmp/chromedriver && \
    chmod 777 /opt/chromedriver-cache /home/appuser/.wdm /tmp/chromedriver && \
    echo "Browser dependencies installed with fallback support for runtime chromium download"

WORKDIR /app

# Stage 2: Install Crawler-specific dependencies
FROM base-crawler AS deps

# Copy only crawler-specific requirements (selenium, newspaper4k, etc.)
COPY requirements-crawler.txt ./

# Install crawler-specific Python packages
RUN pip install --no-cache-dir -r requirements-crawler.txt

# Stage 3: Final runtime image
FROM base-crawler AS runtime

WORKDIR /app

# Copy crawler-specific packages from deps stage
# Base packages and spacy model are already in the base image
COPY --from=deps /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages
COPY --from=deps /usr/local/bin /usr/local/bin
COPY --from=deps /root/.cache /root/.cache

# appuser already created in base image, just set ownership
RUN chown -R appuser:appuser /app

# Copy application code
COPY --chown=appuser:appuser src/ ./src/
COPY --chown=appuser:appuser lookups/ ./lookups/
COPY --chown=appuser:appuser orchestration/ ./orchestration/

# Create data directory for SQLite (will be replaced with Cloud SQL)
RUN mkdir -p /app/data && chown -R appuser:appuser /app/data

# Create bin directory and install chromedriver
RUN mkdir -p /app/bin && \
    # Install chromedriver to match installed chromium version
    # Get chromium version and install matching chromedriver
    CHROMIUM_VERSION=$(chromium --version 2>/dev/null | grep -oP '\d+\.\d+\.\d+\.\d+' | head -1 || echo "unknown") && \
    echo "Detected Chromium version: ${CHROMIUM_VERSION}" && \
    # Download chromedriver for Linux
    # Use latest stable if version detection fails
    if [ "$CHROMIUM_VERSION" != "unknown" ]; then \
        CHROME_MAJOR_VERSION=$(echo $CHROMIUM_VERSION | cut -d. -f1) && \
        echo "Installing chromedriver for Chrome major version: ${CHROME_MAJOR_VERSION}" && \
        wget -q -O /tmp/chromedriver-linux64.zip \
            "https://storage.googleapis.com/chrome-for-testing-public/${CHROME_MAJOR_VERSION}.0.0.0/linux64/chromedriver-linux64.zip" || \
        wget -q -O /tmp/chromedriver-linux64.zip \
            "https://chromedriver.storage.googleapis.com/LATEST_RELEASE_${CHROME_MAJOR_VERSION}/chromedriver_linux64.zip"; \
    else \
        echo "Could not detect Chromium version, installing latest chromedriver" && \
        LATEST_VERSION=$(wget -qO- https://chromedriver.storage.googleapis.com/LATEST_RELEASE) && \
        wget -q -O /tmp/chromedriver-linux64.zip \
            "https://chromedriver.storage.googleapis.com/${LATEST_VERSION}/chromedriver_linux64.zip"; \
    fi && \
    if [ -f /tmp/chromedriver-linux64.zip ]; then \
        apt-get update && apt-get install -y unzip && \
        unzip -j /tmp/chromedriver-linux64.zip -d /app/bin/ '*/chromedriver' || \
        unzip /tmp/chromedriver-linux64.zip -d /tmp/ && mv /tmp/chromedriver /app/bin/ || \
        echo "WARNING: Failed to extract chromedriver from zip"; \
        rm -f /tmp/chromedriver-linux64.zip && \
        chmod +x /app/bin/chromedriver 2>/dev/null && \
        chown appuser:appuser /app/bin/chromedriver 2>/dev/null && \
        echo "ChromeDriver installed successfully" || \
        echo "WARNING: ChromeDriver installation failed, will rely on runtime download"; \
    else \
        echo "WARNING: Could not download chromedriver, will rely on runtime download"; \
    fi && \
    chown -R appuser:appuser /app/bin

# Switch to non-root user
USER appuser

# Set environment variables
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    CHROMEDRIVER_PATH=/app/bin/chromedriver \
    CHROME_BIN=/usr/bin/chromium

# Default command (can be overridden in Kubernetes)
# NOTE: --dataset parameter ensures candidate_links get proper dataset_id assignment
# For Missouri sources, use "Mizzou Missouri State" dataset label
CMD ["python", "-m", "src.cli.main", "discover-urls", "--dataset", "Mizzou Missouri State", "--source-limit", "50"]
