# Standalone Cleaning Command Deployment

**Date:** October 10, 2025  
**Time:** 19:31 UTC  
**Commit:** c933832  
**Build ID:** 5c703762-11e3-4069-b742-7b5ec06ea492  
**Status:** üöß BUILD IN PROGRESS

---

## Problem Summary

**User Question:** "If we have 1,439 articles stuck on 'extracted' why are none being cleaned - that is the next before ML labeling"

**Discovery:** Cleaning is NOT a separate pipeline stage - it's embedded in extraction and only processes articles successfully extracted in that same run.

**Root Cause Chain:**
1. Bot blocking started ~5 days ago
2. Selenium fallback was broken (fixed in d868b99) but still failing
3. All 132 extraction queue articles from bot-blocked domains
4. **0 successful extractions** ‚Üí domains_for_cleaning stays empty
5. **Cleaning never runs** ‚Üí no path from "extracted" to "cleaned"
6. **1,439 articles stuck** with no way to progress
7. ML labeling looks for "cleaned"/"local" ‚Üí finds 0 articles
8. **Pipeline completely stalled** since October 2, 2025

**Architectural Gap:** No standalone cleaning command to process backlog when extraction fails.

---

## Solution: Standalone Cleaning Command

### Created Files

**1. src/cli/commands/cleaning.py (230 lines)**
- Complete standalone command implementation
- Query articles with status "extracted"
- Group by domain, run BalancedBoundaryContentCleaner
- Update status: extracted ‚Üí cleaned/wire/local
- Batch commits every 10 articles
- Full error handling and telemetry

**Command Usage:**
```bash
python -m src.cli.cli_modular clean-articles --limit 50 --status extracted
```

**Key Features:**
- Domain-level analysis (analyze_domain())
- Article-level cleaning (process_single_article())
- Status transitions:
  * `extracted` + local_wire ‚Üí `local`
  * `extracted` + wire ‚Üí `wire`
  * `extracted` + clean ‚Üí `cleaned`
- Progress reporting every 10 articles
- Comprehensive error handling (continue on article failures)

### Modified Files

**2. src/cli/cli_modular.py (+2 lines)**
- Added "clean-articles" to COMMAND_HANDLER_ATTRS
- Added "clean-articles" to command_modules
- Enables lazy loading of cleaning command

**3. orchestration/continuous_processor.py (+33 lines)**
- Added `cleaning_pending` count to get_counts()
  * Query: `SELECT COUNT(*) FROM articles WHERE status = 'extracted' AND content IS NOT NULL`
- Added `process_cleaning(count: int)` function
  * Runs clean-articles command with limit=min(count, 100)
- Added cleaning to main processing loop
  * Priority: After extraction, before analysis
- Fixed ML labeling status filter
  * Added `--status extracted --status cleaned` (was missing extracted)

---

## Build Information

**Build ID:** 5c703762-11e3-4069-b742-7b5ec06ea492  
**Branch:** copilot/investigate-fix-bot-blocking-issues  
**Commit:** c933832  
**Images:**
- `processor:c933832` (commit-specific tag)
- `processor:v1.3.1` (version tag)
- `processor:latest` (rolling tag)

**Build Steps:**
1. ‚è≥ Warm cache (pull latest)
2. ‚è≥ Build processor (with ml-base)
3. ‚è≥ Push processor
4. ‚è≥ Resolve current tags (API & Crawler)
5. ‚è≥ Create Cloud Deploy release

**Expected Duration:** ~1-2 minutes (using ml-base cache)

**Build Log:**
```bash
gcloud builds log 5c703762-11e3-4069-b742-7b5ec06ea492 --stream
```

---

## Deployment Plan

### Phase 1: Build Complete (Expected: 19:33 UTC)
1. ‚úÖ Verify build succeeded
2. ‚úÖ Check release created: `processor-c933832`
3. ‚úÖ Verify image tag exists

### Phase 2: Deploy to Production
```bash
# Promote release
gcloud deploy releases promote \
  --release=processor-c933832 \
  --delivery-pipeline=mizzou-news-crawler \
  --region=us-central1 \
  --to-target=production

# Monitor rollout
gcloud deploy rollouts list \
  --delivery-pipeline=mizzou-news-crawler \
  --region=us-central1 \
  --release=processor-c933832
```

### Phase 3: Verify Deployment
```bash
# Check pod image
kubectl get deployment mizzou-processor -n production \
  -o jsonpath='{.spec.template.spec.containers[0].image}'

# Expected: processor:c933832

# Check pod running
kubectl get pods -n production -l app=mizzou-processor

# Expected: 1/1 Running with age <5m
```

### Phase 4: Monitor Cleaning Cycles
```bash
# Watch logs for cleaning
kubectl logs -n production -l app=mizzou-processor -f | \
  grep -E "(Content cleaning|articles processed|cleaned)"

# Expected output every ~14 minutes:
# ‚ñ∂Ô∏è  Content cleaning (1439 pending, limit 100)
# ‚úÖ Content cleaning completed successfully
# ‚úì Progress: 100/100 articles processed
```

---

## Expected Impact

### Queue Changes (First Hour)

**Before Deployment:**
```
cleaning_pending: N/A (doesn't exist)
analysis_pending: 1,787
entity_extraction_pending: 1,815
```

**After Deployment (15 minutes):**
```
cleaning_pending: 1,439 ‚Üí 1,339 (100 cleaned)
analysis_pending: 1,787 ‚Üí 1,787 (no change yet)
entity_extraction_pending: 1,815 ‚Üí 1,815 (no change)
```

**After Deployment (30 minutes):**
```
cleaning_pending: 1,339 ‚Üí 1,239 (200 cleaned total)
analysis_pending: 1,787 ‚Üí 1,887 (100 new cleaned articles eligible)
entity_extraction_pending: 1,815 ‚Üí 1,815 (no change)
```

### Status Distribution Changes

**Before:**
```sql
extracted: 1,439 articles  ‚Üê STUCK
wire: 193
obituary: 62
opinion: 37
```

**After (1 hour):**
```sql
extracted: ~1,200 articles  ‚Üê Decreasing!
cleaned: ~200 articles      ‚Üê New!
wire: ~210 (193 + ~17)
local: ~10                  ‚Üê New!
obituary: 62
opinion: 37
```

### Pipeline Unblocking

**Impact on ML Labeling:**
- Currently: 0 articles labeled (wrong status filter)
- After deployment: Can label both "extracted" and "cleaned" articles
- Expected: 100+ articles labeled in first hour

**Impact on Entity Extraction:**
- No direct impact (already working)
- Will continue processing extracted articles with content

---

## Success Criteria

### Immediate (15 minutes after deployment)
‚úÖ Pod running with image `processor:c933832`  
‚úÖ Logs show "Content cleaning (1439 pending, limit 100)"  
‚úÖ Logs show "‚úÖ Content cleaning completed successfully"  
‚úÖ Queue count cleaning_pending decreasing  

### Short-term (1 hour)
‚úÖ 200+ articles cleaned (cleaning_pending: 1,439 ‚Üí ~1,200)  
‚úÖ Articles moving to "cleaned" status  
‚úÖ ML labeling processing both extracted and cleaned articles  
‚úÖ Status changes logged: "extracted‚Üícleaned", "extracted‚Üíwire", "extracted‚Üílocal"  

### Medium-term (24 hours)
‚úÖ All 1,439 articles cleaned (cleaning_pending: 0)  
‚úÖ Cleaning running automatically in processor cycles  
‚úÖ ML labeling catching up (1,500+ articles labeled)  
‚úÖ Pipeline operating normally  

---

## Monitoring Queries

### Check Cleaning Progress
```sql
-- Queue status
SELECT 
  COUNT(*) FILTER (WHERE status = 'extracted') as extracted,
  COUNT(*) FILTER (WHERE status = 'cleaned') as cleaned,
  COUNT(*) FILTER (WHERE status = 'wire') as wire,
  COUNT(*) FILTER (WHERE status = 'local') as local
FROM articles
WHERE created_at >= NOW() - INTERVAL '7 days';
```

### Status Transitions Over Time
```sql
-- Articles cleaned in last hour
SELECT 
  DATE_TRUNC('minute', updated_at) as minute,
  status,
  COUNT(*) as status_changes
FROM articles
WHERE updated_at >= NOW() - INTERVAL '1 hour'
AND status IN ('cleaned', 'wire', 'local')
GROUP BY minute, status
ORDER BY minute DESC;
```

### ML Labeling Progress
```sql
-- Labels applied after cleaning deployed
SELECT 
  COUNT(*) as labeled_articles,
  primary_label,
  COUNT(*) as label_count
FROM article_labels
WHERE created_at >= '2025-10-10 19:31:00'  -- Deployment time
GROUP BY primary_label
ORDER BY label_count DESC;
```

### Cleaning Command Performance
```sql
-- Cleaning telemetry (if available)
SELECT 
  created_at,
  metadata->>'domain' as domain,
  metadata->>'chars_removed' as chars_removed,
  metadata->>'wire_detected' as wire_detected
FROM cleaning_telemetry
WHERE created_at >= NOW() - INTERVAL '1 hour'
ORDER BY created_at DESC
LIMIT 50;
```

---

## Rollback Procedure (If Needed)

**If cleaning causes issues:**

1. **Check previous release:**
```bash
gcloud deploy releases list \
  --delivery-pipeline=mizzou-news-crawler \
  --region=us-central1 \
  --limit=5
```

2. **Rollback to processor-df12220:**
```bash
gcloud deploy releases promote \
  --release=processor-df12220 \
  --delivery-pipeline=mizzou-news-crawler \
  --region=us-central1 \
  --to-target=production
```

3. **Verify rollback:**
```bash
kubectl get deployment mizzou-processor -n production \
  -o jsonpath='{.spec.template.spec.containers[0].image}'
```

**Known Issues:**
- None anticipated - cleaning logic already battle-tested in extraction.py
- Same BalancedBoundaryContentCleaner used successfully for months
- Only difference: standalone command vs embedded in extraction

---

## Key Insights

### Why This Was Needed

**Original Architecture (Broken):**
```
Extraction ‚Üí domains_for_cleaning populated ‚Üí Run cleaning on those domains
              ‚Üë
              Only populated when extraction succeeds
              
Result: 0 successful extractions ‚Üí empty domains_for_cleaning ‚Üí no cleaning
```

**New Architecture (Fixed):**
```
Extraction attempts (may fail)
Cleaning runs independently ‚Üí Queries articles with status="extracted"
ML Labeling runs on both "extracted" and "cleaned"

Result: Backlog can progress even when extraction is blocked
```

### Lessons Learned

1. **Pipeline stages should be independent** - Don't couple cleaning to extraction success
2. **Backlog processing is critical** - Need standalone commands for recovery
3. **Status-based processing** - Each stage should query by status, not depend on previous stage success
4. **Monitoring gaps** - Should have noticed 1,439 articles stuck earlier
5. **User questions expose bugs** - "Why are none being cleaned?" revealed architectural gap

### Prevention for Future

‚úÖ **Independent pipeline stages** - Each command queries database by status  
‚úÖ **Backlog recovery tools** - Standalone commands can process stuck articles  
‚úÖ **Status transition monitoring** - Track articles moving through pipeline  
‚úÖ **Queue depth alerts** - Alert when counts stay static for >4 hours  

---

## Timeline

**October 2, 2025 6:00 PM:** Last successful full pipeline completion  
**October 5, 2025:** Bot blocking started affecting all domains  
**October 10, 2025 15:56 UTC:** Deployed bot blocking improvements (5f8ff4b)  
**October 10, 2025 16:10 UTC:** Discovered Selenium not running (rate limit bug)  
**October 10, 2025 16:53 UTC:** Fixed Selenium fallback (d868b99)  
**October 10, 2025 17:15 UTC:** Selenium working but sites still blocking  
**October 10, 2025 18:20 UTC:** Discovered entity extraction infinite loop  
**October 10, 2025 18:47 UTC:** Fixed entity extraction sentinels (df12220)  
**October 10, 2025 19:00 UTC:** User asked: "Why are none being cleaned?"  
**October 10, 2025 19:15 UTC:** Discovered cleaning architectural gap  
**October 10, 2025 19:31 UTC:** Deployed standalone cleaning command (c933832) ‚≠ê

---

## Next Steps

1. **Monitor build completion** (~19:33 UTC)
2. **Promote to production** (~19:35 UTC)
3. **Watch first cleaning cycle** (~19:50 UTC)
4. **Verify queue decreasing** (~20:00 UTC)
5. **Confirm ML labeling working** (~20:30 UTC)
6. **Document full pipeline recovery** (~tomorrow)

---

**Deployment Status:** üöß BUILD IN PROGRESS  
**Build ID:** 5c703762-11e3-4069-b742-7b5ec06ea492  
**Expected Completion:** 19:33 UTC  
**Deployed By:** GitHub Copilot + User Investigation  
**Related Issues:** #64 (Bot Blocking), Pipeline Stalled Since Oct 2
